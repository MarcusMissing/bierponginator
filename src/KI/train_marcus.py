import numpy as npfrom keras.optimizers import Adamfrom keras.preprocessing.image import ImageDataGeneratorfrom sklearn.model_selection import train_test_splitfrom KI.smallervggnet import SmallerVGGNetfrom KI import configepochs = config.epochsbatch_size = config.batch_sizeiteration_learn_rate = config.iteration_learn_rateImage_size = config.image_size_0npz_file_path = '../../resource/preprocessed_images_128.npz'X = np.load(npz_file_path)['X']y = np.array(np.load(npz_file_path)['lables'], dtype='int')(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.2, random_state=42)# construct the image generator for data augmentationaug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,                         horizontal_flip=True, fill_mode='nearest')# initialize the model using a sigmoid activation as the final layer# in the network so we can perform multi-label classificationmodel = SmallerVGGNet.build(    width=Image_size[1], height=Image_size[0],    depth=Image_size[2], classes=10,    finalAct='sigmoid')# initialize the optimizer (SGD is sufficient)opt = Adam(lr=iteration_learn_rate, decay=iteration_learn_rate / epochs)# compile the model using binary cross-entropy rather than# categorical cross-entropy -- this may seem counterintuitive for# multi-label classification, but keep in mind that the goal here# is to treat each output label as an independent Bernoulli# distributionmodel.compile(loss='binary_crossentropy', optimizer=opt,              metrics=['accuracy'])model.fit_generator(    aug.flow(trainX, trainY, batch_size=batch_size),    validation_data=(testX, testY),    steps_per_epoch=len(trainX) // batch_size,    epochs=epochs, verbose=1)model.save('../resource/model.h5')